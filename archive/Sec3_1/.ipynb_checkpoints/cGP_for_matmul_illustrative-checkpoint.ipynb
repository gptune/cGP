{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean everything.\n",
      "Python version:  3.8.8 (default, Apr 13 2021, 19:58:26) \n",
      "[GCC 7.3.0]\n",
      "This is the name of the script:  /home/hrluo/.local/lib/python3.8/site-packages/ipykernel_launcher.py\n",
      "['/home/hrluo/.local/lib/python3.8/site-packages/ipykernel_launcher.py', '-f', '/home/hrluo/.local/share/jupyter/runtime/kernel-7211a0c5-0b7f-4f8e-a6eb-af40cca3da1e.json']\n",
      "numpy version:  1.20.3\n",
      "random stamp for this run: txcrbnsz\n",
      "matplotlib version:  3.3.4\n",
      "GPy version:  1.10.0\n",
      "sklearn version:  0.24.2\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "########################################\n",
    "#      cluster Gaussian surrogate      #\n",
    "########################################\n",
    "#Author: Hengrui Luo\n",
    "#hrluo@lbl.gov\n",
    "#Last update: 2021-Jan-13\n",
    "#\n",
    "########################################\n",
    "#          System information          #\n",
    "########################################\n",
    "#Print the python version and the name/input arguments\n",
    "#%pylab inline\n",
    "import sys\n",
    "print('Clean everything.')\n",
    "sys.modules[__name__].__dict__.clear()\n",
    "import sys\n",
    "print(\"Python version: \", sys.version)\n",
    "print(\"This is the name of the script: \", sys.argv[0])\n",
    "print(sys.argv)\n",
    "\n",
    "#Print the numpy version and set the random seed\n",
    "import numpy as np\n",
    "print('numpy version: ', np.__version__)\n",
    "RND_SEED=111\n",
    "np.random.seed(RND_SEED)\n",
    "\n",
    "#Random string\n",
    "#Get a random string stamp for this specific run, used for the filename of image export.\n",
    "import random\n",
    "import string\n",
    "def get_random_string(length):\n",
    "    return ''.join(random.choice(string.ascii_lowercase) for i in range(length))\n",
    "rdstr=get_random_string(8)\n",
    "print('random stamp for this run:',rdstr)\n",
    "\n",
    "#Print the matplotlib version\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print('matplotlib version: ', matplotlib.__version__)\n",
    "\n",
    "#Print the GPy version\n",
    "import GPy\n",
    "print('GPy version: ', GPy.__version__)\n",
    "\n",
    "#Print the GPy version\n",
    "import sklearn\n",
    "print('sklearn version: ', sklearn.__version__)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "#######################################\n",
    "#         Model specification          #\n",
    "########################################\n",
    "#How many pilot and sequential samples do we allow to get?\n",
    "#N_PILOT is the pilot samples we start with, usually a small number would do.\n",
    "#N_SEQUENTIAL is the number of sequential (noisy) samples we should draw from the black-box function.\n",
    "N_PILOT = 10     #int(sys.argv[1])\n",
    "N_SEQUENTIAL =  90#int(sys.argv[2])\n",
    "#Which method should we use for the Bayesian optimization scheme?\n",
    "#'FREQUENTIST' method means that the (hyper-)parameters are estimated by using some frequestist optimization like lbfgs.\n",
    "#'BAYESIAN' method means that the paramteres are estimated by putting a prior(Gamma)-posterior mechnism, the estimated value would be posterior mean.\n",
    "METHOD  = 'FREQUENTIST'\n",
    "#Following 3 parameters are only for HMC Bayesian sampling, you have to choose METHOD  = 'BAYESIAN' to use these parameters.\n",
    "N_BURNIN = 500\n",
    "N_MCMCSAMPLES = 500\n",
    "N_INFERENCE = 300\n",
    "#Exploration rate is the probability (between 0 and 1) of following the next step produced by acquisition function.\n",
    "EXPLORATION_RATE = 0.5#1.0\n",
    "#Do you want a cluster GP? If NO_CLUSTER = True, a simple GP will be used.\n",
    "NO_CLUSTER = False\n",
    "\n",
    "#Do you want to amplify the weight/role of response X when doing clustering?\n",
    "X_AMPLIFY = 1#/4096\n",
    "#Do you want to subtract an amount from the response X when doing clustering?\n",
    "X_TRANSLATE = []\n",
    "#Do you want to amplify the weight/role of response Y when doing clustering?\n",
    "Y_AMPLIFY = 1#/1000\n",
    "#Do you want to subtract an amount from the response Y when doing clustering?\n",
    "Y_TRANSLATE = 0.\n",
    "#What is the maximal number of cluster by your guess? This option will be used only if NO_CLUSTER=False.\n",
    "N_COMPONENTS = 3\n",
    "#When deciding cluster components, how many neighbors shall we look into and get their votes? This option will be used only if NO_CLUSTER=False.\n",
    "N_NEIGHBORS = 1\n",
    "#Amount of NUGGET in the GP surrogate that stabilize the GP model, especially in FREQUENTIST approach.\n",
    "#NUGGET = 1e-4(Deprecated since ver 0.7, we can use a white kernel to estimate this)\n",
    "#How many time shall we jitter the diagonal of the covariance matrix when we encounter numerical non-positive definiteness in Gaussian process surrogate fitting.\n",
    "#This is a GPy parameter, default is 5 in GPy.\n",
    "N_JITTER = 5\n",
    "#Overriding GPy default jitter, dangerous jittering\n",
    "GPy.util.linalg.jitchol.__defaults__ = (N_JITTER,)\n",
    "print(GPy.util.linalg.jitchol.__defaults__)\n",
    "#This is a GPy parameter, whether you want to normalize the response before/after fitting. Don't change unless necessary.\n",
    "GPy_normalizer = True\n",
    "#Whether we should sample repetitive locations in the sequential sampling procedure.\n",
    "#If True, we would keep identical sequential samples no matter what. (Preferred if we believe a lot of noise)\n",
    "#If False, we would re-sample when we run into identical sequential samples. (Default)\n",
    "#In a acquisition maximization step, this is achieved by setting the acquisition function at repetitive samples to -Inf\n",
    "#In a random search step, this is achieved by repeat the random selection until we got a new location.\n",
    "REPEAT_SAMPLE = False\n",
    "#ver 0.7 new, we can use sklearn GP regression implementation.\n",
    "USE_SKLEARN = True\n",
    "ALPHA_SKLEARN = 1e-5\n",
    "#Value added to the diagonal of the kernel matrix during fitting. \n",
    "SKLEARN_normalizer = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 3)\n",
      "(1000, 1)\n",
      "(1000, 1)\n",
      "Closest point in dataset is  128  with observed value  2000.758\n",
      "2000.758\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "#   Example 4: record dataset mapping, matmul    #\n",
    "##################################################\n",
    "EXAMPLE_NAME='matmul'\n",
    "#This loads the dataset for building a black-box function\n",
    "#The format of the dataset should be a csv file, the first column being the response (Y)\n",
    "#The rest columns of the dataset is the d-dimensional inputs (X)\n",
    "#\n",
    "from numpy import genfromtxt\n",
    "my_data = genfromtxt('Giulia_1000.csv', delimiter=',')\n",
    "print(my_data.shape)\n",
    "my_data = np.delete(my_data, (0), axis=0)\n",
    "Y_obs = my_data[:,2].astype(float).reshape(-1,1)\n",
    "X_obs = my_data[:,1].astype(float).reshape(-1,1)\n",
    "#Dimension of the input domain\n",
    "#d = X_obs.shape[1]\n",
    "print(X_obs.shape)\n",
    "print(Y_obs.shape)\n",
    "\n",
    "########################################\n",
    "#          Function wrapping           #\n",
    "########################################\n",
    "#This allows us to wrap a real-world dataset into the format of a black-box function useful \n",
    "#Given a point X, we find the closest point X' in the dataset (by some distance measure, currently L^2).\n",
    "#The black-box function would return the observed response value Y' for X'. \n",
    "#This wrapping would makes the black-box function to be piece-wise constant. \n",
    "#\n",
    "from scipy.spatial.distance import cdist\n",
    "def f_truth(X):\n",
    "    to_obs = cdist(X,X_obs, metric='euclidean')\n",
    "    closest_obs = np.argmin(to_obs)\n",
    "    ret_X = X_obs[closest_obs,:]\n",
    "    ret_Y = Y_obs[closest_obs,:]\n",
    "    ret_X = int(X)\n",
    "    #print(np.argwhere(ret_X==X_obs))\n",
    "    #ret_Y = Y_obs[np.argwhere(ret_X==X_obs)[0,0],:]\n",
    "    ret_Y = Y_obs[np.argmin(np.abs(ret_X-X_obs) ),:]\n",
    "    print('Closest point in dataset is ',ret_X,' with observed value ',ret_Y[0])\n",
    "    return ret_Y[0].astype(float)\n",
    "point1 = np.ones((1,1))*128.0\n",
    "print(f_truth(point1))\n",
    "bounds = np.array([[1,1000]]).astype(float)\n",
    "#print(bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_GRID = 1024\n",
    "x_p = [None]*bounds.shape[0]\n",
    "for i in range(bounds.shape[0]):\n",
    "    x_p[i] = np.linspace(start=bounds[i,0], stop=bounds[i,1], num=N_GRID)\n",
    "    x0grid_ravel = np.vstack(np.meshgrid( *x_p )).reshape(bounds.shape[0],-1).T\n",
    "    x0grid_ravel = np.arange(0,4096+1,8)\n",
    "    x0grid_ravel = x0grid_ravel.astype(float).reshape(-1,1)\n",
    "#You must supply a parameter called 'bounds'.\n",
    "inp_dim=bounds.shape[0]\n",
    "\n",
    "#Which kernel you want to use for your model? Such a kernel must be implemented as a GPy/sklearn kernel class.\n",
    "if USE_SKLEARN==True:\n",
    "    from sklearn.gaussian_process import *\n",
    "    KERNEL_TEMPLATE = sklearn.gaussian_process.kernels.Matern(length_scale=np.ones(inp_dim,), length_scale_bounds=(1e-05, 100000.0), nu=3/2) + sklearn.gaussian_process.kernels.WhiteKernel(noise_level=1.0, noise_level_bounds=(1e-03, 1000.0))\n",
    "    #KERNEL_TEMPLATE = sklearn.gaussian_process.kernels.Matern(length_scale=np.ones(inp_dim,), length_scale_bounds=(1e-05, 100000.0), nu=1/2)\n",
    "else:\n",
    "    KERNEL_TEMPLATE = GPy.kern.Matern32(input_dim=inp_dim, variance=1., lengthscale=1.) + GPy.kern.White(input_dim=inp_dim)\n",
    "    #KERNEL_TEMPLATE = GPy.kern.Exponential(input_dim=inp_dim, variance=1., lengthscale=1.)\n",
    "#Do you want to penalize boundary sample points? If so, how?\n",
    "def boundary_penalty(X,data_X=None):\n",
    "    #return 0\n",
    "    #return np.zeros((X.shape[0],1)) #if you don't want any penalty, use this line as the definition of your penalty\n",
    "    #ret = []\n",
    "    #for g in range(X.shape[0]):\n",
    "    #    g_list = []\n",
    "    #    for h in range(bounds.shape[1]):\n",
    "    #        g_list.append( np.sum( (X[g,:]-bounds[:,h])**2 ) )\n",
    "    #    ret.append(min(g_list))\n",
    "    #res = X.astype(int)%8==0\n",
    "    #return res*(100)\\\n",
    "    #if X<100:\n",
    "    #    return -1e5\n",
    "    if X.astype(int)%8==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1e3\n",
    "    return -1e3\n",
    "def censor_function(Y):\n",
    "    #return Y #if you don't want any censor, use this line as the definition of your censor function.\n",
    "    ret = Y\n",
    "    #ret = Y.*(Y<20000 & Y>100)\n",
    "    return ret#-np.minimum(0.1,10/np.asarray(ret))\n",
    "#ver 0.6 new, \n",
    "#if random_domain returns TRUE, then such a choice by the random step is acceptable.\n",
    "#if random_domain returns FALSE, then such a choice is out of our search input domain, and we would like to re-sample another random location.\n",
    "def random_domain(X,data_X=None):\n",
    "    #return True\n",
    "    for i in range(data_X.shape[0]):\n",
    "        if all(X.astype(int)== data_X[i,:].astype(int)) and ~REPEAT_SAMPLE: return False\n",
    "        #This is only for matmul example searching only multiples of 8.\n",
    "    return X.astype(int)%8==0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# datetime object containing current date and time\n",
    "samplestartingtime = datetime.now()\n",
    "########################################\n",
    "#          Draw pilot samples          #\n",
    "########################################\n",
    "#This cell only provides a pilot sample.\n",
    "#Prepare pilot samples (X,Y)\n",
    "print('\\n>>>>>>>>>>Sampling ',N_PILOT,' pilot samples...<<<<<<<<<<\\n')\n",
    "print('Example : ',EXAMPLE_NAME)\n",
    "X_sample = np.zeros((N_PILOT,bounds.shape[0]))\n",
    "Y_sample = np.zeros((N_PILOT,1))\n",
    "for j in range(bounds.shape[0]):\n",
    "    X_sample[:,j] = np.random.uniform(bounds[j,0],bounds[j,1],size=(N_PILOT,1)).ravel()\n",
    "Y_sample = np.zeros((N_PILOT,1))\n",
    "for k in range(N_PILOT):\n",
    "    Y_sample[k,0] = f_truth(X_sample[k,:].reshape(1,-1))\n",
    "    Y_sample[k,0] = censor_function(Y_sample[k,0])\n",
    "#print('Pilot X',X_sample)\n",
    "#print('Pilot Y',Y_sample)\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#The cGP procedure consists of following steps\n",
    "#Step 1. For observations, we can do a (unsupervised) (X,Y)-clustering and label them, different components are generated.\n",
    "#Step 2. For predictive locations, we can do a (supervised) k-nearest neighbor classification, and predict at each location based on which component it belongs to. \n",
    "#Step 3. We compute the acquisition function and then proceed to the next sample, after adding the new sample we repeat Step 1 and 2.\n",
    "\n",
    "#Prepare an up-to-date X_TRANSLATE, as the empirical mean of the X_sample\n",
    "if len(X_TRANSLATE)>0:\n",
    "    X_TRANSLATE = np.mean(X_sample,axis=0)\n",
    "#Prepare an up-to-date Y_TRANSLATE, as the empirical mean of the Y_sample\n",
    "if Y_TRANSLATE != 0:\n",
    "    Y_TRANSLATE = np.mean(Y_sample)\n",
    "#print(Y_sample - Y_TRANSLATE)\n",
    "#Prepare initial clusters, with XY-joint.\n",
    "XY_sample       = np.concatenate((X_AMPLIFY*(X_sample-X_TRANSLATE),Y_AMPLIFY*(Y_sample-Y_TRANSLATE).reshape(-1,1)),axis=1)\n",
    "#dgm_XY = BayesianGaussianMixture(\n",
    "#                    #weight_concentration_prior_type=\"dirichlet_distribution\",\n",
    "#                    weight_concentration_prior_type=\"dirichlet_process\",\n",
    "#                    n_components=N_COMPONENTS,#pick a big number, DGM will automatically adjust\n",
    "#                    )\n",
    "dgm_XY = KMeans(n_clusters=N_COMPONENTS, random_state=0)\n",
    "XY_label = dgm_XY.fit_predict(XY_sample)\n",
    "print('\\n Initial labels for (X,Y)-joint clustering',XY_label)\n",
    "#Make copies of X_sample for X-only fitting and XY-joint fitting.\n",
    "X_sample_XY = np.copy(X_sample)\n",
    "Y_sample_XY = np.copy(Y_sample)\n",
    "#Prepare initial labels\n",
    "clf_XY = KNeighborsClassifier(n_neighbors=N_NEIGHBORS)\n",
    "clf_XY.fit(X_sample_XY, XY_label)\n",
    "#This is an artifact, we need to have at least d samples to fit a d-dimensional GP model (for its mean and variance)\n",
    "for c in np.unique(XY_label):\n",
    "    if sum(XY_label==c)<=X_sample_XY.shape[1]:\n",
    "        occ = np.bincount(XY_label)\n",
    "        XY_label[np.where(XY_label==c)] = np.argmax(occ)\n",
    "\n",
    "        \n",
    "print(X_sample,Y_sample)\n",
    "print(XY_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "#        Draw sequential samples       #\n",
    "########################################\n",
    "from scipy import stats\n",
    "from matplotlib import cm\n",
    "mycm = cm.Spectral\n",
    "VERBOSE = False\n",
    "GETPLOT = False\n",
    "\n",
    "#Prepare sequential samples (X,Y)\n",
    "print('\\n>>>>>>>>>>Sampling ',N_SEQUENTIAL,' sequential samples...<<<<<<<<<<\\n')\n",
    "X_sample = X_sample_XY\n",
    "Y_sample = Y_sample_XY\n",
    "cluster_label = XY_label\n",
    "\n",
    "def get_KER():\n",
    "    return KERNEL_TEMPLATE\n",
    "#This recode function will turn the labels into increasing order,e.g. [1, 1, 3, 3, 0] ==> [0, 0, 1, 1, 2].\n",
    "def recode(label):\n",
    "    level = np.unique(np.array(label))\n",
    "    ck = 0\n",
    "    for j in level:\n",
    "        label[label==j]=ck\n",
    "        ck=ck+1\n",
    "    return label\n",
    "#Main loop that guides us in sampling sequential samples\n",
    "comp_l = np.unique(np.array(cluster_label))\n",
    "\n",
    "for it in range(N_SEQUENTIAL):\n",
    "    print('\\n>>>>>>>>>> ***** STEP ',it+1,'/',N_SEQUENTIAL,'***** <<<<<<<<<<')\n",
    "    #Step 1. For observations, we can do a (unsupervised) (X,Y)-clustering and label them, different components are generated.\n",
    "    #Create the (X,Y) joint sample to conduct (unsupervised clustering)\n",
    "    if len(X_TRANSLATE)>0:\n",
    "        X_TRANSLATE = np.mean(X_sample,axis=0)\n",
    "    if Y_TRANSLATE != 0:\n",
    "        Y_TRANSLATE = np.mean(Y_sample)\n",
    "    #The cluster must be based on adjusted response value Y.\n",
    "    XY_sample        = np.concatenate((X_AMPLIFY*(X_sample-X_TRANSLATE),Y_AMPLIFY*(Y_sample-Y_TRANSLATE).reshape(-1,1)),axis=1)\n",
    "    if NO_CLUSTER:\n",
    "        print('>>NO CLUSTER, a GP surrogate.')\n",
    "        cluster_label    = np.zeros(XY_sample.shape[0])\n",
    "    else:\n",
    "        print('>>CLUSTERED, a cGP surrogate.',len(comp_l),' components in surrogate model.')\n",
    "        cluster_label    = dgm_XY.fit_predict(XY_sample)#cluster_label\n",
    "        if VERBOSE: print('dgm label', cluster_label)\n",
    "        #Again, we need to ensure that every cluster has at least d (dimension of covariate) samples.\n",
    "        for c in np.unique(cluster_label):\n",
    "            if sum(cluster_label==c)<=X_sample.shape[1]:\n",
    "                occ = np.bincount(cluster_label)\n",
    "                cluster_label[np.where(cluster_label==c)] = np.argmax(occ)\n",
    "        if VERBOSE: print('merged label',cluster_label)\n",
    "    cluster_label = recode(cluster_label)\n",
    "    if VERBOSE: print('All labels are recoded: ',cluster_label)\n",
    "    #Create arrays to store the mean&variance at observed locations and predictive locations.\n",
    "    n_component=len(np.unique(cluster_label))\n",
    "    mean_fun = np.zeros((len(cluster_label),1))\n",
    "    var_fun = np.copy(mean_fun)\n",
    "    \n",
    "    #Step 2. For predictive locations, we can do a (supervised) k-nearest neighbor classification, and predict at each location based on which component it belongs to. \n",
    "    clf_XY = KNeighborsClassifier(n_neighbors=N_NEIGHBORS)\n",
    "    clf_XY.fit(X_sample,cluster_label)\n",
    "        \n",
    "    #Step 3. We either randomly search one location or compute the acquisition function and then proceed to the next sample, after adding the new sample we repeat Step 1 and 2.\n",
    "    coin = np.random.uniform(0,1,1)\n",
    "    if coin<EXPLORATION_RATE:\n",
    "        print('>>>>Find next sample: acquisition proposal.')\n",
    "        comp_l = np.unique(np.array(cluster_label))\n",
    "        for c in comp_l:\n",
    "            #Assign the corresponding X_sample and Y_sample values to the cluster coded by c. \n",
    "            c_idx = np.where(cluster_label == int(c))\n",
    "            if VERBOSE: \n",
    "                print('>>>>Fitting component ',c,'/',len(comp_l)-1,' total components')\n",
    "                print(c_idx)\n",
    "            Xt = X_sample[c_idx].ravel().reshape(-1,X_sample.shape[1])\n",
    "            Yt = Y_sample[c_idx].ravel().reshape(-1,1)\n",
    "            #Fit the model with normalization\n",
    "            if USE_SKLEARN==True:\n",
    "                mt = GaussianProcessRegressor(kernel=get_KER(), random_state=0, normalize_y=SKLEARN_normalizer,alpha=ALPHA_SKLEARN,  \n",
    "                                              optimizer='fmin_l_bfgs_b', n_restarts_optimizer=int(10*bounds.shape[0]))\n",
    "            else:\n",
    "                mt = GPy.models.GPRegression(Xt, Yt, kernel=get_KER(), normalizer=GPy_normalizer)\n",
    "            ###\n",
    "            if METHOD == 'FREQUENTIST':\n",
    "                ##############################\n",
    "                #Frequentist MLE GP surrogate#\n",
    "                ##############################\n",
    "                print('>>>>>>METHOD: frequentist MLE approach, component '+str(c)+'/'+str(len(comp_l)-1))\n",
    "                print('>>>>>>SAMPLE: component sample size =',len(c_idx[0]) )\n",
    "                if USE_SKLEARN==True:\n",
    "                    mt.fit(Xt, Yt)\n",
    "                    #No need to do more for sklearn GP\n",
    "                    print('>>>>>>MODULE: sklearn is used, l-bfgs optimization.')\n",
    "                    if VERBOSE: print(mt.kernel_, mt.log_marginal_likelihood(mt.kernel_.theta))\n",
    "                else:\n",
    "                    print('>>>>>>MODULE: GPy is used, l-bfgs optimization.')\n",
    "                    mt.optimize(optimizer='bfgs', gtol = 1e-100, messages=VERBOSE, max_iters=int(10000*bounds.shape[0]))\n",
    "                    mt.optimize_restarts(num_restarts=int(10*bounds.shape[0]),robust=True,verbose=VERBOSE)\n",
    "            elif METHOD == 'BAYESIAN':\n",
    "                if USE_SKLEARN: sys.exit('FUTURE: Currently we cannot fit with Bayesian method using sklearn, we have GPy only.')\n",
    "                ##############################\n",
    "                #Fully Bayesian GP surrogate #\n",
    "                ##############################\n",
    "                #Prior on the \"hyper-parameters\" for the GP surrogate model.\n",
    "                print('>>>>>>METHOD: Fully Bayesian approach, component '+str(c)+'/'+str(len(comp_l)-1))\n",
    "                print('>>>>>>SAMPLE: component sample size =',len(c_idx[0]) )\n",
    "                mt.kern.lengthscale.set_prior(GPy.priors.Gamma.from_EV(1.,10.))\n",
    "                mt.kern.variance.set_prior(GPy.priors.Gamma.from_EV(1.,10.))\n",
    "                #HMC sampling, fully Bayesian approach to estimate the kernel parameters.\n",
    "                hmc = GPy.inference.mcmc.HMC(mt,stepsize=0.1)\n",
    "                s = hmc.sample(num_samples=N_BURNIN) # Burnin\n",
    "                s = hmc.sample(num_samples=N_MCMCSAMPLES)\n",
    "                MCMC_samples = s[N_INFERENCE:] # cut out the burn-in period\n",
    "                # Set the model parameters as the posterior mean\n",
    "                mt.kern.variance[:]    = MCMC_samples[:,0].mean()\n",
    "                mt.kern.lengthscale[:] = MCMC_samples[:,1].mean()\n",
    "            #######################################\n",
    "            # Optimization module(each component) #\n",
    "            #######################################\n",
    "            #mt2 predicts on observed locations.\n",
    "            #No matter GRID_SEARCH true or not, we still need to predict on observed locations\n",
    "            if USE_SKLEARN:\n",
    "                mt2 = mt.predict(Xt,return_std=True, return_cov=False)\n",
    "                mean_fun[c_idx,0] = mean_fun[c_idx,0] + mt2[0].reshape(1,-1)\n",
    "                var_fun[c_idx,0]  = var_fun[c_idx,0]  + mt2[1].reshape(1,-1)\n",
    "            else:\n",
    "                mt2 = mt.predict(Xt)\n",
    "                mean_fun[c_idx,0] = mean_fun[c_idx,0] + mt2[0].reshape(1,-1)#*np.std(Yt) + np.mean(Yt)\n",
    "                var_fun[c_idx,0]  = var_fun[c_idx,0]  + mt2[1].reshape(1,-1)#*np.std(Yt)*np.std(Yt)\n",
    "            #Define the expected improvement as objective function to optimize over.\n",
    "            def my_obj(X):\n",
    "                my_X = X.reshape(1, -1)\n",
    "                my_X_label = clf_XY.predict(my_X)\n",
    "                #If not in this component, set it to zero immediately.\n",
    "                if my_X_label != int(c): return -0\n",
    "                my_xi = 0.0 #tuning parameter, set it to zero for now.\n",
    "                if USE_SKLEARN:\n",
    "                    my_gp = mt.predict(my_X, return_std=True, return_cov=False)\n",
    "                    my_mu = my_gp[0]\n",
    "                    my_sigma = my_gp[1]\n",
    "                else:\n",
    "                    my_gp = mt.predict(my_X)\n",
    "                    my_mu = my_gp[0]\n",
    "                    my_sigma = my_gp[1]\n",
    "                    my_sigma = np.sqrt(np.absolute(my_sigma)).reshape(-1, 1)\n",
    "                my_mu = np.asarray(my_mu)\n",
    "                my_sigma = np.asarray(my_sigma)\n",
    "                with np.errstate(divide='warn'):\n",
    "                    my_imp = my_mu - np.max(mt2[0].reshape(1,-1)) - my_xi\n",
    "                    my_Z = np.divide(my_imp,my_sigma)\n",
    "                    #norm = mvn(mean=np.zeros(X_sample[0,:].shape), cov=np.eye(X_sample.shape[1]))\n",
    "                    my_ei = my_imp * norm.cdf(my_Z) + my_sigma * norm.pdf(my_Z)\n",
    "                    my_ei[np.where(my_sigma <= 0.0)] = 0.0\n",
    "                #Here we penalize the acquisition function value according to boundary_penalty function, by default this would be disabled. See document for details.\n",
    "                my_ei = my_ei + boundary_penalty(my_X,X_sample)\n",
    "                my_ei = float(my_ei.ravel())\n",
    "                if VERBOSE: print('EI=',my_ei,'\\n')\n",
    "                return - my_ei/Xt.shape[0]\n",
    "            #Optimize this my_obj using some optimization method.\n",
    "            from scipy.optimize import minimize\n",
    "            #from scipy.optimize import dual_annealing\n",
    "            func = my_obj#lambda x:my_obj(x,mt,clf_XY) #Since the anneal finds minimum\n",
    "            lw = bounds[:,0].tolist()\n",
    "            up = bounds[:,1].tolist()\n",
    "            #ret = dual_annealing(func, bounds=list(zip(lw, up)), seed=123)\n",
    "            #dual annealing works for dim=1\n",
    "            ret = minimize(fun=func, x0=np.random.uniform(bounds[:,0].T,bounds[:,1].T), bounds=list(zip(lw, up)), method='L-BFGS-B')\n",
    "            print('>>>>Maximal acquisition function = ',-ret.fun,' attained at ',ret.x,' for component ',c)\n",
    "            X_next = ret.x\n",
    "    else:\n",
    "        print('>>>>Find next sample: random search.')\n",
    "        randomize_counter = 0\n",
    "        X_rand = np.zeros((1,bounds.shape[0]))\n",
    "        for j in range(bounds.shape[0]):\n",
    "            X_rand[0,j] = np.random.uniform(bounds[j,0],bounds[j,1],1)\n",
    "        X_next = X_rand\n",
    "        #If we do not want repetitive samples, we sample until there are no points nearby. \n",
    "        while ~random_domain(X_next,X_sample):\n",
    "            if VERBOSE: print('Random search: ',X_next,'hits a repetitive sample OR does not hit the random_domain constraint, resampling...')\n",
    "            X_rand = np.zeros((1,bounds.shape[0]))\n",
    "            for j in range(bounds.shape[0]):\n",
    "                X_rand[0,j] = np.random.uniform(bounds[j,0],bounds[j,1],1)\n",
    "            X_next = X_rand\n",
    "            randomize_counter = randomize_counter + 1\n",
    "        print('>>>>Random search stops after ',randomize_counter,' steps.')\n",
    "    #Optional: Following are plotting features that tracks the optimization procedure\n",
    "    X_next = X_next.reshape(1,-1)\n",
    "    Y_next = f_truth(X_next)\n",
    "    print('----------')\n",
    "    print('>>Next sample input is chosen to be: ',X_next)\n",
    "    print('>>Next sample response is chosen to be: ',Y_next.ravel())\n",
    "    if GETPLOT:\n",
    "        X_new = x0grid_ravel\n",
    "        if bounds.shape[0]==1:\n",
    "            fig, axs = plt.subplots(2,figsize=(6,6))\n",
    "            fig.suptitle('Fitted surrogate model, sample size = '+str(X_sample.shape[0]))\n",
    "            axs[0].plot(X_new,mean_new,color='b')\n",
    "            axs[0].scatter(X_sample,Y_sample,color='b')\n",
    "            axs[0].set_title('observed samples and mean')\n",
    "            ci = np.sqrt(var_new)#/mean_new\n",
    "            axs[0].fill_between(X_new.ravel(), (mean_new-ci).ravel(), (mean_new+ci).ravel(), color='b', alpha=.1)\n",
    "            axs[1].plot(fine_grid,ei_grid,color='k')\n",
    "            axs[1].scatter(X_next,ei_next,marker='v',color='r',s=100)\n",
    "            axs[1].text(s='x='+str(X_next),x=X_next,y=np.max(ei_grid),color='r',fontsize=12)\n",
    "            axs[1].set_title('acquisition/expected improvement function')\n",
    "            plt.show()\n",
    "        if bounds.shape[0]==2:\n",
    "            fig, axs = plt.subplots(2,figsize=(6,12))\n",
    "            fig.suptitle('Fitted surrogate model, sample size = '+str(X_sample.shape[0]))\n",
    "            axs[0].scatter(X_new[:,0],X_new[:,1],c=mean_new.ravel(),cmap=mycm)\n",
    "            axs[0].scatter(X_sample[:,0],X_sample[:,1],c=Y_sample.ravel(),cmap=mycm,marker='v',s=200,edgecolors='k')\n",
    "\n",
    "            axs[0].set_title('observed samples and mean')\n",
    "            ci = np.sqrt(var_new)#/mean_new\n",
    "            axs[1].scatter(fine_grid[:,0],fine_grid[:,1],c=ei_grid.ravel(),cmap=mycm)\n",
    "            axs[1].scatter(X_next[0,0],X_next[0,1],marker='v',color=None,s=200,edgecolors='k')\n",
    "            axs[1].text(s='x='+str(X_next),x=X_next[0,0],y=X_next[0,1],color='k',fontsize=12)\n",
    "            axs[1].set_title('acquisition/expected improvement function')\n",
    "            plt.show()\n",
    "        #plt.savefig('cGP'+rdstr+'_step'+str(it)+'_'+str(n)+'_'+str(m)+'_'+str(l)+'.png')\n",
    "    #Update X and Y from this step.\n",
    "    X_sample = np.vstack((X_sample,X_next))\n",
    "    Y_sample = np.vstack((Y_sample,censor_function(Y_next) ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleendingtime = datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "samplestartingtime  = samplestartingtime.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "sampleendingtime  = sampleendingtime.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "print(\"Sample start date and time =\", samplestartingtime)\n",
    "print(\"Sample end date and time =\", sampleendingtime)\n",
    "#print(X_sample)\n",
    "#print(Y_sample)\n",
    "#print(np.hstack((Y_sample,X_sample)).shape)\n",
    "if NO_CLUSTER==True:\n",
    "    FILE_NAME = EXAMPLE_NAME+'_local_GP('+rdstr+')'\n",
    "else:\n",
    "    FILE_NAME = EXAMPLE_NAME+'_local_cGP_k='+str(N_COMPONENTS)+'('+rdstr+')'\n",
    "np.savetxt(FILE_NAME+'.txt', np.hstack((Y_sample,X_sample)), delimiter =', ')  \n",
    "\n",
    "sample_max_x = X_sample[np.argmax(Y_sample),:] \n",
    "sample_max_f = np.round( Y_sample[np.argmax(Y_sample),:],3)\n",
    "sample_min_x = X_sample[np.argmin(Y_sample),:] \n",
    "sample_min_f = np.round( Y_sample[np.argmin(Y_sample),:],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "    with open(FILE_NAME+'.log', 'w') as f:\n",
    "        sys.stdout = f # Change the standard output to the file we created.\n",
    "        #print('This message will be written to a file.')\n",
    "        print(\"Example: \",EXAMPLE_NAME,file=f)\n",
    "        print(\"Sample start date and time = \", samplestartingtime)\n",
    "        print(\"Sample end date and time = \", sampleendingtime)\n",
    "        print(\"Python version: \", sys.version)\n",
    "        #print(\"Filename of the script: \", sys.argv[0])\n",
    "        print(\"Commandline arguments: \",sys.argv)\n",
    "        print(\"Random seed: \",RND_SEED)\n",
    "        print('Random stamp: ',rdstr)\n",
    "        print('GPy version: ', GPy.__version__)\n",
    "        print('sklearn version: ', sklearn.__version__)\n",
    "        print('Number of pilot samples: ',N_PILOT)\n",
    "        print('Number of sequential samples: ',N_SEQUENTIAL)\n",
    "        print('Surrogate fitting method: ',METHOD)\n",
    "        if METHOD==\"BAYESIAN\":\n",
    "            print('MCMC>Burn-in steps: ',N_BURNIN)\n",
    "            print('MCMC>Sampling steps: ',N_MCMCSAMPLES)\n",
    "            print('MCMC>Inference sample length: ',N_INFERENCE)\n",
    "        print('Surrogate> Are we using sklearn for GPR?: ',USE_SKLEARN)\n",
    "        print('Surrogate> kernel type: ',get_KER())\n",
    "        if USE_SKLEARN:\n",
    "            print('Surrogate>sklearn>jittering: ',ALPHA_SKLEARN)\n",
    "            print('Surrogate>sklearn>normalizer; ',SKLEARN_normalizer)\n",
    "        else:\n",
    "            #print('Surrogate>GPy>Nugget noise variance',NUGGET)\n",
    "            print('Surrogate>GPy>jittering: ',N_JITTER)\n",
    "            print('Surrogate>GPy>normalizer; ',GPy_normalizer)\n",
    "        print('Surrogate> Fit a simple GP?(no cluster): ',NO_CLUSTER)\n",
    "        print('Cluster> Response amplifier when clustering: ',Y_AMPLIFY)\n",
    "        print('Cluster> Maximal number of components/clusters: ',N_COMPONENTS)\n",
    "        print('Classify> k in k-nearest neighbor classifier',N_NEIGHBORS)\n",
    "        print('Exploration rate: ',EXPLORATION_RATE)\n",
    "        #print('Exploration> Do we perform grid-search in acquisition maximization?',GRID_SEARCH)\n",
    "        print('Exploration> Do we allow repeat samples in random searching?',REPEAT_SAMPLE)\n",
    "        print('domain bounds: ',bounds)\n",
    "        #print('blur amount: ',blur_amount)\n",
    "        print('sample minimum, f_min=',sample_min_f,' at ',sample_min_x)\n",
    "        print('sample maximum, f_max=',sample_max_f,' at ',sample_max_x)\n",
    "        print('>>Cluster X_AMPLIFY=',X_AMPLIFY)\n",
    "        print('>>Cluster X_TRANSLATE=',X_TRANSLATE)\n",
    "        print('>>Cluster Y_AMPLIFY=',Y_AMPLIFY)\n",
    "        print('>>Cluster Y_TRANSLATE=',Y_TRANSLATE)\n",
    "    sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "\n",
    "#%debug\n",
    "import os\n",
    "print('Logs of run with stamp: ',rdstr,', is saved at',os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_label    = dgm_XY.fit_predict(XY_sample)#cluster_label\n",
    "prediction_label = clf_XY.predict(x0grid_ravel)#XY_predlabel\n",
    "print('dgm label', cluster_label)\n",
    "#Again, we need to ensure that every cluster has at least d (dimension of covariate) samples.\n",
    "for c in np.unique(cluster_label):\n",
    "    if sum(cluster_label==c)<=X_sample.shape[1]:\n",
    "        occ = np.bincount(cluster_label)\n",
    "        cluster_label[np.where(cluster_label==c)] = np.argmax(occ)\n",
    "print('merged label',cluster_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "#      Plot the final model(1/2D)      #\n",
    "########################################\n",
    "mycm = cm.coolwarm\n",
    "X_new = x0grid_ravel\n",
    "fine_grid = x0grid_ravel\n",
    "prediction_label = clf_XY.predict(x0grid_ravel)\n",
    "new_label = clf_XY.predict(X_new)\n",
    "col=['r','k','y','b','g'] #Generate a color scale, here usually there would not be more than 5 components.\n",
    "mean_new = np.zeros((len(prediction_label),1))\n",
    "var_new = np.copy(mean_new)\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "#from IPython.display import display\n",
    "if len(X_TRANSLATE)>0:\n",
    "    X_TRANSLATE = np.mean(X_sample,axis=0)\n",
    "if Y_TRANSLATE != 0:\n",
    "    Y_TRANSLATE      = np.mean(Y_sample)\n",
    "XY_sample        = np.concatenate((X_AMPLIFY*(X_sample-X_TRANSLATE),Y_AMPLIFY*(Y_sample-Y_TRANSLATE).reshape(-1,1)),axis=1)\n",
    "#XY_sample        = np.concatenate((X_sample,Y_AMPLIFY*Y_sample.reshape(-1,1)),axis=1)\n",
    "if NO_CLUSTER: \n",
    "    cluster_label    = np.zeros(XY_sample.shape[0])\n",
    "    prediction_label = x0grid_ravel*0.\n",
    "else:\n",
    "    cluster_label    = dgm_XY.fit_predict(XY_sample)#cluster_label\n",
    "    prediction_label = clf_XY.predict(x0grid_ravel)#XY_predlabel\n",
    "    if VERBOSE: print('dgm label', cluster_label)\n",
    "    #Again, we need to ensure that every cluster has at least d (dimension of covariate) samples.\n",
    "    for c in np.unique(cluster_label):\n",
    "        if sum(cluster_label==c)<=X_sample.shape[1]:\n",
    "            occ = np.bincount(cluster_label)\n",
    "            cluster_label[np.where(cluster_label==c)] = np.argmax(occ)\n",
    "    if VERBOSE: print('merged label',cluster_label)\n",
    "cluster_label = recode(cluster_label)\n",
    "clf_XY = KNeighborsClassifier(n_neighbors=N_NEIGHBORS)\n",
    "clf_XY.fit(X_sample,cluster_label)\n",
    "#if GRID_SEARCH==True:\n",
    "new_label = clf_XY.predict(X_new)\n",
    "for c in np.unique(cluster_label):\n",
    "            if sum(cluster_label==c)<=X_sample.shape[1]:\n",
    "                occ = np.bincount(cluster_label)\n",
    "                cluster_label[np.where(cluster_label==c)] = np.argmax(occ)\n",
    "cluster_label = recode(cluster_label)\n",
    "print(cluster_label)\n",
    "new_label = recode(new_label)\n",
    "print(new_label)\n",
    "for c in np.unique(np.array(cluster_label)):\n",
    "        print('Fitting component ',c)\n",
    "        c = int(c)\n",
    "        #Assign the corresponding X_sample and Y_sample values to the cluster coded by c. \n",
    "        c_idx = np.where(cluster_label == int(c))\n",
    "        if len(c_idx) <1: continue\n",
    "        print(c_idx)\n",
    "        Xt = X_sample[c_idx].ravel().reshape(-1,X_sample.shape[1])\n",
    "        Yt = Y_sample[c_idx].ravel().reshape(-1,1)\n",
    "        #print(Xt.shape,Yt.shape)\n",
    "        #print(Xt,Yt)\n",
    "        #Normalization\n",
    "        #Fit the model\n",
    "        if 'mt' in locals():\n",
    "            del(mt)\n",
    "            # mt exists.\n",
    "        if USE_SKLEARN:\n",
    "            mt = GaussianProcessRegressor(kernel=get_KER(), random_state=0, normalize_y=SKLEARN_normalizer, alpha=ALPHA_SKLEARN,  \n",
    "                                              optimizer='fmin_l_bfgs_b', n_restarts_optimizer=int(10*bounds.shape[0]))\n",
    "            mt.fit(Xt, Yt)\n",
    "            print('Summary of component '+str(c)+' GP surrogate model.')\n",
    "            print(mt.kernel_, mt.log_marginal_likelihood(mt.kernel_.theta))\n",
    "\n",
    "        else:\n",
    "            mt = GPy.models.GPRegression(Xt, Yt, kernel=get_KER(), normalizer=GPy_normalizer)\n",
    "            mt.optimize(optimizer='bfgs', gtol = 10e-32, messages=False, max_iters=int(10000*bounds.shape[0]))\n",
    "            mt.optimize_restarts(num_restarts=int(100*bounds.shape[0]),robust=True,verbose=False)\n",
    "            #mt.plot()\n",
    "            #plt.show()\n",
    "            print('Summary of component '+str(c)+' GP surrogate model.')\n",
    "            display(mt)\n",
    "        c_idx_new = np.where(new_label == int(c))\n",
    "        c_idx_new = c_idx_new[0]\n",
    "        if len(c_idx_new) <1: continue\n",
    "        print(c_idx_new)\n",
    "        #print(mean_new.shape)\n",
    "        if USE_SKLEARN:\n",
    "            mt1 = mt.predict(X_new[c_idx_new],return_std=True, return_cov=False)\n",
    "            mt2 = mt.predict(fine_grid,return_std=True, return_cov=False)\n",
    "            mu_new = mt1[0]\n",
    "            sigma2_new = np.power(mt1[1],2)\n",
    "        else:\n",
    "            mt1 = mt.predict(X_new[c_idx_new])\n",
    "            mt2 = mt.predict(fine_grid)\n",
    "            mu_new = mt1[0]\n",
    "            sigma2_new = mt1[1]\n",
    "            \n",
    "        mean_new[c_idx_new,0] = mean_new[c_idx_new,0] + mu_new.reshape(1,-1)\n",
    "        var_new[c_idx_new,0]  = var_new[c_idx_new,0]  + sigma2_new.reshape(1,-1)\n",
    "        \n",
    "        if bounds.shape[0] == 1:\n",
    "            plt.scatter(X_new[c_idx_new],np.ones(X_new[c_idx_new].shape)*0+50,c=col[c],alpha=1,marker='s',s=100)\n",
    "            #plt.plot(fine_grid, mt2[0],color=col[c],linestyle='--',label='component '+str(c)+' mean')\n",
    "            plt.scatter(X_sample[c_idx],   Y_sample[c_idx],label='sequential samples',c=col[c],alpha=0.5)\n",
    "plt.plot(X_obs,Y_obs,c='m')\n",
    "plt.vlines(x=112, ymin=0, ymax=35000,color='g',linewidth=10,alpha=0.5)\n",
    "\n",
    "if bounds.shape[0] == 1:\n",
    "    print('1d plot')\n",
    "    plt.plot(X_new,mean_new,color='b',linewidth=4,alpha=0.5,label='overall mean')\n",
    "    plt.fill_between(X_new.ravel(), (mean_new-np.sqrt(var_new)).ravel(), (mean_new+np.sqrt(var_new)).ravel(), color='b', alpha=.1, label='overall std. deviation')\n",
    "   \n",
    "    #plt.vlines(x=sample_max_x, ymin=0, ymax=sample_max_f,color='b',linestyle='-.')\n",
    "    #plt.text(s='sample max:'+str(sample_max_f[0])+'\\n @'+str(sample_max_x),x=sample_max_x,y=100,c='k',fontsize=12,rotation=45)\n",
    "    #plt.text(s=str(sample_max_x[0]),x=sample_max_x,y=20,c='b',fontsize=12)\n",
    "\n",
    "    ##plt.vlines(x=sample_min_x, ymin=0, ymax=sample_min_f,color='b',linestyle='-.')\n",
    "    #plt.text(s='sample min:'+str(sample_min_f[0])+'\\n @'+str(sample_min_x),x=sample_min_x,y=100,c='k',fontsize=12,rotation=45)\n",
    "    #plt.text(s=str(sample_min_x[0]),x=sample_min_x,y=10,c='b',fontsize=12)\n",
    "\n",
    "    plt.title('Sample size ='+str(N_PILOT)+'+'+str(N_SEQUENTIAL)+'='+str(X_sample.shape[0])+', '+str(len(np.unique(np.array(cluster_label))))+' components.'+\\\n",
    "              '\\n f_max='+str(sample_max_f[0])+', x_max='+str(np.round(sample_max_x[0])),fontsize=32)\n",
    "    plt.ylabel('Y', fontsize=24)\n",
    "    plt.xlabel('X', fontsize=24)\n",
    "    plt.xlim((0,1001))\n",
    "    plt.ylim((0,2000))\n",
    "    plt.xticks(np.linspace(0, 1000, 9), fontsize=24)\n",
    "    plt.yticks(np.linspace(0, 2000, 6), fontsize=24)\n",
    "    #plt.legend(fontsize=18,loc='lower center')\n",
    "    \n",
    "if bounds.shape[0] == 2:\n",
    "    print('2d plot')\n",
    "    plt.scatter(X_new[:,0],      X_new[:,1],      c=mean_new.ravel(),cmap=mycm,alpha=1.0,label='overall mean',marker='s',s=200)\n",
    "    plt.scatter(X_sample[:,0],   X_sample[:,1],   c=Y_sample.ravel(),cmap=mycm,alpha=1.0,label='sequential samples',edgecolors='k')\n",
    "    plt.scatter(X_sample_XY[:,0],X_sample_XY[:,1],c=Y_sample_XY.ravel(),cmap=mycm,alpha=1.0,label='pilot samples',marker='v',s=150,edgecolors='k')\n",
    "    #plt.scatter(x=x_min[0], y=x_min[1], color='k')\n",
    "    #plt.text(s='model min:'+str(f_min[0])+'\\n @'+str(x_min),x=x_min[0],y=x_min[1],c='k',fontsize=12,rotation=45)\n",
    "\n",
    "    #plt.scatter(x=x_max[0], y=x_max[1], color='k')\n",
    "    #plt.text(s='model max:'+str(f_max[0])+'\\n @'+str(x_max),x=x_max[0],y=x_max[1],c='k',fontsize=12,rotation=45)\n",
    "\n",
    "    #plt.scatter(x=sample_max_x[0], y=sample_max_x[1], color='k')\n",
    "    #plt.text(s='sample max:'+str(sample_max_f[0])+'\\n @'+str(sample_max_x),x=sample_max_x[0],y=sample_max_x[1],c='k',fontsize=12,rotation=45)\n",
    "    #plt.text(s=str(sample_max_x[0]),x=sample_max_x,y=20,c='b',fontsize=12)\n",
    "\n",
    "    #plt.scatter(x=sample_min_x[0], y=sample_min_x[1], color='k')\n",
    "    #plt.text(s='sample min:'+str(sample_min_f[0])+'\\n @'+str(sample_min_x),x=sample_min_x[0],y=sample_min_x[1],c='k',fontsize=12,rotation=45)\n",
    "    #plt.text(s=str(sample_min_x[0]),x=sample_min_x,y=10,c='b',fontsize=12)\n",
    "\n",
    "    #plt.title('Sample size ='+str(X_sample.shape[0]),fontsize=24)\n",
    "    plt.xlabel('X1', fontsize=24)\n",
    "    plt.ylabel('X2', fontsize=24)\n",
    "    plt.xlim((-1,1))\n",
    "    plt.ylim((-1,1))\n",
    "    plt.xticks(np.linspace(-1, 1, 6), fontsize=24)\n",
    "    plt.yticks(np.linspace(-1, 1, 6), fontsize=24)\n",
    "    \n",
    "    #plt.legend()\n",
    "\n",
    "plt.ylim((0,2100))\n",
    "#plt.yticks(ticks=[21000,23000,25000,27000,29000,31000])\n",
    "plt.show()\n",
    "fig.savefig(FILE_NAME+'.png', dpi=fig.dpi)\n",
    "print('sample minimum, f_min=',sample_min_f,' at ',sample_min_x)\n",
    "print('sample maximum, f_max=',sample_max_f,' at ',sample_max_x)\n",
    "print('>>Cluster X_AMPLIFY=',X_AMPLIFY)\n",
    "print('>>Cluster X_TRANSLATE=',X_TRANSLATE)\n",
    "print('>>Cluster Y_AMPLIFY=',Y_AMPLIFY)\n",
    "print('>>Cluster Y_TRANSLATE=',Y_TRANSLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.concatenate((X_AMPLIFY*(X_sample-X_TRANSLATE),Y_AMPLIFY*(Y_sample-Y_TRANSLATE).reshape(-1,1)),axis=1))\n",
    "print(np.concatenate((X_sample,Y_AMPLIFY*(Y_sample-0.).reshape(-1,1)),axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
